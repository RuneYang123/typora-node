### Spark详解（一、Spark概述）

Spark是一种基于内存的快速的、通用、可拓展的大数据分析计算引擎。

#### 一、Spark与MapReduce

Hadoop框架中的MapReduce计算引擎，也是一种大数据分析计算引擎。那既然已经又来MR那我们为何还要开发Spark计算模型呢？或者说这两者有何相同之处？在应用方面有何不同？

##### 1、Spark与Hadoop

Hadoop是一个开源的生态圈，有文件储存的HDFS，有计算引擎MR，有资源调度的YARN，以及数据库的Hbase等等。

Spark就仅仅是一个计算引擎，仅仅相对的是MR。并不能和hadoop来做比较。

##### 2、Spark与MR

首先来说，MR程序是由Java开发，在使用时使用Java编写程序。Spark是由Scala语言开发，使用Scala编写程序。（Scala也是一种基于JVM的函数式编程语言）

Spark是一个数据快速的分析项目，他的核心技术是弹性分布式数据集（RDD）。提供了比MR丰富的模型，可以快速在内存中对数据集进行多次迭代。不像MR，如果要进行复杂的计算，需要多个MR程序相串联。

Spark是一个基于内存的计算引擎，而MR是一个基于磁盘的操作。这也是他们最大的区别，MR多个作业都要依赖于磁盘交互，但是Spark就不用，只需要在写入的时候进行一次。所以Spark在大部分情况下比MR要快。

#### 二、Spark结构

Spark是一个标准分布式计算引擎，采用master-slave结构。

##### 1、核心组件

**Driver**：是Spark的驱动器节点，负责实际代码的执行工作。将用户程序转化成作业（job），在Executor之间调度任务（task）等等。

**Executor**：在Spark中运行具体的任务（Task），每个Executor相互独立。如果一个任务中有Executor故障，任务会到其他节点运行。

**Master&Worke**r：在Spark独立部署环境中，不需要依赖其他的调度框架，自己就可以通过Master&Worker自己王城资源的调度。Master主要负责资源的调度，Worker就是安装在每个节点上的负责这个节点的计算。其实就类似与Yarn上的RM和NM。

**ApplicationMaster**：Hadoop用户在向yarn提交任务时，提交程序中应该包括ApplicationMaster，用于对资源容器的申请，监控任务的执行，跟踪任务的状态，处理任务的失败。

##### 2、核心概念

**RDD**：弹性分布式数据集，他是Spark中最基本的数据处理文件。弹性是指，储存内存与磁盘的自动切换，数据丢失可以自动恢复等。数据集是指封装了计算逻辑，并不保存数据。不可变，RDD封装了计算逻辑，是不可变更的，想要改变只能产生新的RDD。

**DGA（有向无环图）**：是Spark程序直接映射成的数据流的高级抽象模型。简单理解就是程序计算执行的过程图形表示出来，有划分的阶段以及执行的方向。

